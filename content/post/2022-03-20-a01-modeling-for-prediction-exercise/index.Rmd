---
title: 'A01: Modeling for Prediction Exercise'
author: ''
date: '2022-03-20'
slug: a01-modeling-for-prediction-exercise
categories: []
tags: []
---
# Questions
### a) LOOCV is better than k-fold cross validation in some cases where the most accurate estimate of the machine learning model is neccesary since it reduces bias by using all the data. However, since it is essentially a k-fold cross validation method where the number of partitions is equal to the sample size it will take up a lot of time and computer resources making it a model that scales with a dataset size very poorly. In the same vein single split validation will be worse than k fold cross validation in all cases where k>2 because a single split validation is essentially just saying split the data into two groups: one model and one test. If there is significant variation between the groups in terms of data then the single split validation will preform poorly. Essentially in terms of accuracy for cross validation it goes single split, k-fold, and LOOCV but the amount of time and resources needed also increases in that order.
### b)The advantages of bootstrapping is that it does not really on assuming the shape of the population since it is constantly drawing samples from the data one provided to make its infernce(bootstrapping model assumes this sampling with repetition is similar to sampling from the population. However, for this reason it will work poorly on small data sets since it cannot extrapolate any info from the data provided (whereas other models may assume the shape of the population and if this assumption is wrong may produce a wrong sampling statistic)
### c) Getting data from github below
### d)For the k-fold cross validation method x1-x4 were variables that were tried and x5, x6 were ignored as latitude and longitude without any context(i.e. country or city) would not be a good variable. A loop was ran to determine the error for K values 1-10 and each graph contained info for polynomial degree 1-10 which was done for each variable. Based on this, I saw that for the variable x4, generally speaking the error was generally less and the range of error for each of the polynomial degrees was less as well. Furthermore, some of the other plots plateaued the error near 0 as the degrees went near 6-10 which was a sign of overfitting. For x4, the error was the lowest near degree = 6 and for this reason I believe the x4 variable or number of convenience stores with a sixth degree fitting with a K of 10 is the best model.
### e) For the boostrapping validation method,looking at the histogram x4 variable seemed to have the histogram with the distribution looking most like the normal distribution and this did not change much with sample size unless going to the extremes such as sample size of 10. Furthermore, once the degree for the polynomial got to 5-7 there was not much variation between these graphs and this means our results above from k fold were corrborated and we obtained the equation for x4 at degree 6 fitting.
# Setup
```{r}
library(ISLR)
library(boot)
library(dplyr)
library(readr)

real_estate <- read_csv("https://raw.githubusercontent.com/vsomesula/COPwebsite/main/content/post/2022-03-20-a01-modeling-for-prediction-exercise/Real%20estate%20valuation%20data%20set.csv")

real_estate <- rename(real_estate, "x1" = 'X1 transaction date', "x2" = 'X2 house age', "x3" = 'X3 distance to the nearest MRT station', "x4" = 'X4 number of convenience stores', "x5" = 'X5 latitude', "x6" = 'X6 longitude', y = 'Y house price of unit area' )
attach(real_estate)
```
# K fold Cross Validation
### Ignore x5 and x6 i.e. lattitude and longitude as without context(city, rural, etc) it does not mean anything

```{r}
#var_cycle <- c(x1, x2, x3, x4)
KValue <- 2:10
for(K in KValue){
degree <- 1:10  
cv.error <- rep(0, max(degree))
for(d in degree){
 # for(v in var_cycle){
  glm.fit <- glm(y~poly(x4, d), data =  real_estate)
  cv.error[d] <- cv.glm(real_estate, glm.fit, K = K)$delta[1]
  #}
}
cv.error
plot(degree, cv.error, type = "b")
lines(degree, cv.error, type = "b", col = "red")
}
```
# BootStrap Validation
## Estimation of accuracy of a polynomial model
```{r}

boot.fn <- function(data, index){
  return(coef(glm(y~poly(x4, 6), data = data, subset = index)))
}
set.seed(1)
boot.fn(real_estate, sample(414, 414, replace = T))
boot.out <- boot(real_estate, boot.fn, 500)
plot(boot.out)
```










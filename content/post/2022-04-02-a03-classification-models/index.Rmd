---
title: a03 classification models
author: ''
date: '2022-04-02'
slug: a03-classification-models
categories: []
tags: []
---
# Executive Summary
### The Titanic ship sinking was a great tragedy in history. The tragedy has generated data on which passenger survived and which did not. In addition to this data, other data has been collected for each passenger such as sex,name, and fare amongst other identifiers. The goal is to build a logistic model that predicts which passengers survive and which die.
# Issues and challenges
### The immediate challenge is to pick which variables to use for this logistic regression model as not all variables will be high yield predictors.
# Setup
```{r}
if(!require("pacman")) install.packages("pacman")
pacman::p_load(pacman,readxl, magrittr, productplots, psych, RColorBrewer, tidyverse, boot, InformationValue,caret)
train <- read_csv("https://raw.githubusercontent.com/vsomesula/COPwebsite/main/content/post/2022-04-02-a03-classification-models/train.csv")
test <- read_csv("https://raw.githubusercontent.com/vsomesula/COPwebsite/main/content/post/2022-04-02-a03-classification-models/test.csv")
submission <- read_csv("https://raw.githubusercontent.com/vsomesula/COPwebsite/main/content/post/2022-04-02-a03-classification-models/submission.csv")
attach(train)
```
# Exploratory Data Analyis
### We gain a sense of what the variables are in this dataset. Some variables we can automatically excluse just from a cursory glance of the dataset such as Name, PassengerId, Ticket, and Cabin.
```{r}
glimpse(train)
```
# Build a logistic model with all the variables
### Doin so allows us to gain the p-values for each variable allowing us to determine which variables are high-yield predictors and which are not
```{r}
model <- glm(formula = Survived~Pclass+Sex+Age+SibSp+Parch+Fare+Embarked, data = train, family = 'binomial')
summary(model)
```
### Using the p-values for each of the variables we can see how Pclass, Sex, Age, and SibSP, are very good predictors of surival rate, Embarked is a mediocre predictor and Fare is a poor predictor. 
### To further determine the order of which variable is a good predictor and which is a poor predictor we can use the caret package.
```{r}
caret::varImp(model)
```
### A higher value means more importance for that variable. With this information, we can see that the order of how important each variable is Sex, Pclass, Age, and SibSp. Embarked could be included but is not likly to add much value (Especially, since if we obserbe the p-values above they are both above 0.1 which mains most scientists would consider them not significant). Additionally, Parch and Fare can be completly excluded as their values are much lower than the good predictors such as Pclass.

### It is important to note that since not all passengers have age data we can not include this important variable in those model. Having that data point would cause there to be NA's for some of the passangers who do not have thar variable.

# Building a logistic model with all variables determined to be important
```{r}
model <- glm(formula = Survived~Pclass+Sex+SibSp, data = train, family = 'binomial')
summary(model)
```
### Once again, we can obserbe the p-values being signficant for the variables we selected. We can see that this model is a better predictor than a model with all the variables since the AIC is lower for the new model.

# Applying the model to the test data set to predict survivors
#### The data may not display properly due to the long names of the passengners, however, the output is a dataframe with dimensions 418 x 3.
```{r}
predictions <- predict(model, test, type = "response")
output <- data.frame(test$PassengerId, test$Name, Survived = predictions )
output
```
### We can see the survival probability for each of the passengers along with their name and passenger ID.

### However, having the probablity of surviving is not very useful on its own. The power of a logistic model is to determine a binary output which in this case if the passenger survives. Therefore, we run a command that converts all probability of surviving below 50% to not survived or a 0, and all probabilities above to survived or a 1.
```{r}
output$Survived = ifelse(output$Survived > 0.5, 1, 0)
output
```
### To determine, the power of our model we can compare our output dataframe against the submission csv file generated from the kaggle python script.
```{r}
compare <- data.frame(1:418)
compare <- ifelse(output$Survived == submission$Survived, 1, 0)
compare <- as.numeric(compare)
table(compare)
```
### We see that 409 of the passangers match between the Kaggle and the model I have developed which suggests that my solution is fairly accurate. Furthermore, the Kaggle model uses the Parch variable which I have excluded only resulting in 9 differnces suggesting that the Parch variable is not a very high-yield predictor.


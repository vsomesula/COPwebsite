---
title: a03 classification models
author: ''
date: '2022-04-02'
slug: a03-classification-models
categories: []
tags: []
---
# Executive Summary
### The Titanic ship sinking was a great tragedy in history. The tragedy has generated data on which passenger survived and which did not. In addition to this data, other data has been collected for each passenger such as sex,name, and fare amongst other identifiers. The goal is to build a logistic model that predicts which passengers survive and which die.
# Issues and challenges
### The immediate challenge is to pick which variables to use for this logistic regression model as not all variables will be high yield predictors. 

**There are two dataframes that are long inserted into the webpage. These dataframes are put into the solution so that the reader may observe the rawoutput and then how it was transformed using a threshold**

# Setup
```{r}
if(!require("pacman")) install.packages("pacman")
pacman::p_load(pacman,readxl, magrittr, productplots, psych, RColorBrewer, tidyverse, boot, InformationValue,caret)
train <- read_csv("https://raw.githubusercontent.com/vsomesula/COPwebsite/main/content/post/2022-04-02-a03-classification-models/train.csv")
test <- read_csv("https://raw.githubusercontent.com/vsomesula/COPwebsite/main/content/post/2022-04-02-a03-classification-models/test.csv")
submission <- read_csv("https://raw.githubusercontent.com/vsomesula/COPwebsite/main/content/post/2022-04-02-a03-classification-models/submission.csv")
attach(train)
```
# Exploratory Data Analyis
### We gain a sense of what the variables are in this dataset. Some variables we can automatically excluse just from a cursory glance of the dataset such as Name, PassengerId, Ticket, and Cabin. (because these are all unique per passanger)
```{r}
glimpse(train)
```

```{r}
train %>%
  ggplot() +
  geom_bar(aes(Pclass, fill =  Pclass)) +
  theme(
    axis.title.x = element_blank(),
    legend.position = "none"
  )
count(train, Pclass)
```
### We can see a decent distribution of the Pclass variable. However, there seems to be quite a lot of people in Pclass 3.There are almost the same amount of people in 3 as 1 and 2 put together.

```{r}
train %>%
  ggplot() +
  geom_bar(aes(Sex, fill =  Sex)) +
  theme(
    axis.title.x = element_blank(),
    legend.position = "none"
  )
count(train, Sex)
```
### There are many more males than females

```{r}
train %>%
  ggplot() +
  geom_bar(aes(Age, fill =  Age)) +
  theme(
    axis.title.x = element_blank(),
    legend.position = "none"
  )
count(train, Age)
```
### Age as a variable seems to be clustered around 25-30 range and the overall distribution seems to be right skewed.
```{r}
train %>%
  ggplot() +
  geom_bar(aes(SibSp, fill =  SibSp)) +
  theme(
    axis.title.x = element_blank(),
    legend.position = "none"
  )
count(train, SibSp)
```
### THere are a lot of people with zero siblings. This data could be collapsed into a binary that is either you have a sibling or you do not.
```{r}
train %>%
  ggplot() +
  geom_bar(aes(Parch, fill =  Parch)) +
  theme(
    axis.title.x = element_blank(),
    legend.position = "none"
  )
count(train, Parch)
```
### This is similar to the Sibsp variable. It cann be transformed into either you have 0 parch or you have more than 0.

```{r}

train %>%
  ggplot() +
  geom_bar(aes(Fare, fill =  Fare)) +
  theme(
    axis.title.x = element_blank(),
    legend.position = "none"
  )
count(train, Fare)

```
### This data could be transformed as looking at the graph and data we can see there are some very low fare below 20 dollars some around 50 and some very expensive ones in the 100 and above. Categorizing this data could improve the predicting power.
```{r}
train %>%
  ggplot() +
  geom_bar(aes(Embarked, fill =  Embarked)) +
  theme(
    axis.title.x = element_blank(),
    legend.position = "none"
  )
count(train, Embarked)
```
### There is a decent distribution of this ordinal data. No point in transforming since only three categories to begin with.


# Build a logistic model with all the variables
### Doin so allows us to gain the p-values for each variable allowing us to determine which variables are high-yield predictors and which are not
```{r}
model <- glm(formula = Survived~Pclass+Sex+Age+SibSp+Parch+Fare+Embarked, data = train, family = 'binomial')
summary(model)
```
### Using the p-values for each of the variables we can see how Pclass, Sex, Age, and SibSP, are very good predictors of surival rate, Embarked is a mediocre predictor and Fare is a poor predictor. 
### To further determine the order of which variable is a good predictor and which is a poor predictor we can use the caret package.
```{r}
caret::varImp(model)
```
### A higher value means more importance for that variable. With this information, we can see that the order of how important each variable is Sex, Pclass, Age, and SibSp. Embarked could be included but is not likly to add much value (Especially, since if we obserbe the p-values above they are both above 0.1 which mains most scientists would consider them not significant). Additionally, Parch and Fare can be completly excluded as their values are much lower than the good predictors such as Pclass.

### It is important to note that since not all passengers have age data we can not include this important variable in those model. Having that data point would cause there to be NA's for some of the passangers who do not have thar variable.

# Building a logistic model with all variables determined to be important
```{r}
model <- glm(formula = Survived~Pclass+Sex+SibSp, data = train, family = 'binomial')
summary(model)
```
### Once again, we can obserbe the p-values being signficant for the variables we selected. We can see that this model is a better predictor than a model with all the variables since the AIC is lower for the new model.

# Applying the model to the test data set to predict survivors
#### The data may not display properly due to the long names of the passengners, however, the output is a dataframe with dimensions 418 x 3.
```{r}
predictions <- predict(model, test, type = "response")
output <- data.frame(test$PassengerId, test$Name, Survived = predictions )
output
```
### We can see the survival probability for each of the passengers along with their name and passenger ID.

### However, having the probablity of surviving is not very useful on its own. The power of a logistic model is to determine a binary output which in this case if the passenger survives. Therefore, we run a command that converts all probability of surviving below 50% to not survived or a 0, and all probabilities above to survived or a 1.
```{r}
output$Survived = ifelse(output$Survived > 0.5, 1, 0)
output
```
### To determine, the power of our model we can compare our output dataframe against the submission csv file generated from the kaggle python script.
```{r}
compare <- data.frame(1:418)
compare <- ifelse(output$Survived == submission$Survived, 1, 0)
compare <- as.numeric(compare)
table(compare)
```
### We see that 409 of the passangers match between the Kaggle and the model I have developed which suggests that my solution is fairly accurate. Furthermore, the Kaggle model uses the Parch variable which I have excluded only resulting in 9 differnces suggesting that the Parch variable is not a very high-yield predictor.

# To determine sensitivity and specifity we can use confusion matrix
```{r}
classificationtable<- table(output$Survived, submission$Survived)
sensitivity<-(classificationtable[2,2]/(classificationtable[2,2]+classificationtable[2,1]))*100
 sensitivity
 
 specificity<-(classificationtable[1,1]/(classificationtable[1,1]+classificationtable[1,2]))*100
 specificity 
```
### Both sensitivity and specificity are high as we can see from the values which means this moddle is a very good model.
